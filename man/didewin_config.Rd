% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/config.R
\name{didewin_config}
\alias{didewin_config}
\alias{didewin_config_global}
\title{Configuration}
\usage{
didewin_config(credentials = NULL, home = NULL, temp = NULL,
  cluster = NULL, build_server = NULL, shares = NULL, template = NULL,
  cores = NULL, wholenode = NULL, parallel = NULL, hpctools = NULL,
  workdir = NULL, use_workers = NULL, use_rrq = NULL,
  worker_timeout = NULL, rtools = NULL, r_version = NULL)

didewin_config_global(...)
}
\arguments{
\item{credentials}{Either a list with elements username, password,
or a path to a file containing lines \code{username=<username>}
and \code{password=<password>} or your username (in which case
you will be prompted graphically for your password).}

\item{home}{Path to network home directory, on local system}

\item{temp}{Path to network temp directory, on local system}

\item{cluster}{Name of the cluster to use (one of
\code{\link{valid_clusters}()})}

\item{build_server}{Currently ignored, but will soon be a windows
build server for bespoke binary packages.}

\item{shares}{Optional additional share mappings.  Can either be a
single path mapping (as returned by \code{\link{path_mapping}}
or a list of such calls.}

\item{template}{A job template.  On fi--dideclusthn this can be
"GeneralNodes", "4Core" or "8Core", while on "fi--didemrchnb"
this can be "GeneralNodes", "12Core" or "16Core", "12and16Core",
"20Core", or "24Core".  For fi--didelxhn the only valid template
is "LinuxNodes".  See the main cluster documentation if you
tweak these parameters, as you may not have permission to use
all templates (and if you use one that you don't have permission
for the job will fail).}

\item{cores}{The number of cores to request.  This is mostly
useful when using the \code{GeneralNodes} or \code{LinuxNodes}
template.  If specified, then we will request this many cores
from the windows queuer.  If you request too many cores then
your task will queue forever!  8 is the largest this should be
on fi--didehusthn and 16 on fi--didemrchnb (while there are 20
core nodes you may not have access to them).  If omitted then a
single core is selected for the GeneralNodes template or the
\emph{entire machine} for the other templates (unless modified
by \code{wholenode}).}

\item{wholenode}{Request the whole node?  This will default to
\code{TRUE} if any template other than \code{GeneralNodes} is
selected.}

\item{parallel}{Should we set up the parallel cluster?  Normally
if more than one core is implied (via the \code{cores} argument,
by picking a template other than \code{GeneralNodes} or by using
\code{wholenode}) then a parallel cluster will be set up (see
Details).  If \code{parallel} is set to \code{FALSE} then this
will not occur.  This might be useful in cases where you want to
manage your own job level parallelism (e.g. using OpenMP) or if
you're just after the whole node for the memory).}

\item{hpctools}{Use HPC tools if available?}

\item{workdir}{The path to work in on the cluster, if running out of place.}

\item{use_workers}{Submit jobs to an internal queue, and run them
on a set of workers submitted separately?  If \code{TRUE}, then
\code{enqueue} and the bulk submission commands no longer submit
to the DIDE queue.  Instead they create an \emph{internal} queue
that workers can poll.  After queuing tasks, use
\code{submit_workers} to submit workers that will process these
tasks, terminating when they are done.  You can use this
approach to throttle the resources you need.}

\item{use_rrq}{Use \code{rrq} to run a set of workers on the
cluster.  This is an experimental option, and the interface here
may change.  For now all this does is ensure a few additional
packages are installed, and tweaks some environment variables in
the generated batch files.  Actual rrq workers are submitted
with the \code{submit_workers} method of the object.}

\item{worker_timeout}{When using workers (via \code{use_workers}
or \code{use_rrq}, the length of time (in seconds) that workers
should be willing to set idle before exiting.  If set to zero
then workers will be added to the queue, run jobs, and
immediatly exit.  If greater than zero, then the workers will
wait at least this many seconds after running the last task
before quitting.  The number provoided can be `Inf`, in which
case the worker will never exit (but be careful to clean the
worker up in this case!).  The default is 600s (10 minutes)
should be more than enough to get your jobs up and running.
Once workers are established you can extend or reset the timeout
by sending the \code{TIMEOUT_SET} message (proper documentation
will come for this soon).}

\item{rtools}{Make sure that rtools are installed (even if they
aren't implicitly required by one of the required packages).  If
\code{TRUE}, then network paths will be set up appropriately
such that R on the cluster should find the appropriate version
of rtools so that packages such as \code{rstan} and
\code{Rcpp}'s inline functionality work correctly.}

\item{r_version}{A string, or \code{numeric_version} object,
describing the R version required.  Not all R versions are known
to be supported, so this will check against a list of installed
R versions for the cluster you are using (see
\code{r_versions}).  If omitted then: if your R version matches
a version on the cluster that will be used, or the oldest
cluster version that is newer than yours, or the most recent
cluster version.}

\item{...}{arguments to \code{didewin_config}}
}
\description{
Collects configuration information.  Unfortunately there's a
fairly complicated process of working out what goes where so
documentation coming later.
}
\section{Resources and parallel computing}{


If you need more than one core per task (i.e., you want the each
  task to do some parallel processing \emph{in addition} to the
  parallelism between tasks) you can do that through the
  configuration options here.

The \code{template} option choses among templates defined on the
  cluster.  If you select one of these then we will reserve an
  entire node \emph{unless} you also specify \code{cores}.
  Alternatively if \code{wholenode} is specified this overrides
  the logic here.

If you specify \code{cores}, the HPC will queue your job until an
  appropriate number of cores appears for the selected template.
  This can leave your job queing forever (e.g., selecting 20 cores
  on a 16Core template) so be careful.  The \code{cores} option is
  most useful with the \code{GeneralNodes} template, which is the
  default (\code{LinuxNodes} on \code{fi--didelxhn}).

In either case, if more than 1 core is implied (either by using
  any template other than \code{GeneralNodes} or by specifying a
  \code{cores} value greater than 1) on startup, a \code{parallel}
  cluster will be started, using \code{parallel::makePSOCKcluster}
  and this will be registered as the default cluster.  The nodes
  will all have the appropriate context loaded and you can
  immediately use them with \code{parallel::clusterApply} and
  related functions by passing \code{NULL} as the first argument.
  The cluster will be shut down politely on exit, and logs will be
  output to the "workers" directory below your context root.
}

\section{Workers and rrq}{


The options \code{use_workers} and \code{use_rrq} interact, share
some functionality, but are quite different.

With \code{use_workers}, jobs are never submitted when you run
\code{enqueue} or one of the bulk submission commands in
\code{queuer}.  Instead you submit workers using
\code{submit_workers} and then the submission commands push task
ids onto a Redis queue that the workers monitor.

With \code{use_rrq}, \code{enqueue} etc still work as before, plus
you \emph{must} submit workers with \code{submit_workers}.  The
difference is that any job may access the \code{rrq_controller}
and push jobs onto a central pool of tasks.

I'm not sure at this point if it makes any sense for the two
approaches to work together so this is disabled for now.  If you
think you have a use case please let me know.
}

