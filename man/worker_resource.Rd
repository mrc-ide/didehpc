% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/config.R
\name{worker_resource}
\alias{worker_resource}
\title{Specify worker resources}
\usage{
worker_resource(
  template = NULL,
  cores = NULL,
  wholenode = NULL,
  parallel = NULL
)
}
\arguments{
\item{template}{A job template.  On fi--dideclusthn this can be
"GeneralNodes" or "8Core". On "fi--didemrchnb" this can be
"GeneralNodes", "12Core", "16Core", "12and16Core", "20Core",
"24Core", "32Core", or "MEM1024" (for nodes with 1Tb of RAM; we have
three - two of which have 32 cores, and the other is the AMD epyc with
64). On the new "wpia-hpc-hn" cluster, you should
currently use "AllNodes". See the main cluster documentation if you
tweak these parameters, as you may not have permission to use
all templates (and if you use one that you don't have permission
for the job will fail).  For training purposes there is also a
"Training" template, but you will only need to use this when
instructed to.}

\item{cores}{The number of cores to request.  This is mostly
useful when using the \code{GeneralNodes} template.  If
specified, then we will request this many cores from the windows
queuer.  If you request too many cores then your task will queue
forever!  24 is the largest this should be on fi--dideclusthn,
64 on fi--didemrchnb and 32 on wpia-hpc-hn (assuming you have access to those
nodes).  If omitted then a single core is selected for the
GeneralNodes template or the \emph{entire machine} for the other
templates (unless modified by \code{wholenode}).}

\item{wholenode}{Request the whole node?  This will default to
\code{TRUE} if any template other than \code{GeneralNodes} is
selected.}

\item{parallel}{Should we set up the parallel cluster?  Normally
if more than one core is implied (via the \code{cores} argument,
by picking a template other than \code{GeneralNodes} or by using
\code{wholenode}) then a parallel cluster will be set up (see
Details).  If \code{parallel} is set to \code{FALSE} then this
will not occur.  This might be useful in cases where you want to
manage your own job level parallelism (e.g. using OpenMP) or if
you're just after the whole node for the memory).}
}
\value{
A list with class \code{worker_resource} which can be passed
into \code{\link{didehpc_config}}
}
\description{
Specify resources for worker processes. If given, the values here
will override those in \code{\link[=didehpc_config]{didehpc_config()}}. See
\code{vignette("workers")} for more details.
}
